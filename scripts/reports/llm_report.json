{
  "id": "cmpl-nmhgmrqcbilw3pc77tgu3f",
  "object": "text_completion",
  "created": 1737825836,
  "model": "dolphincoder-starcoder2-7b",
  "choices": [
    {
      "index": 0,
      "text": "\n\nHere is the expected output:\n{\"findings\":[{\"file_path\":\"../src/app.js\",\"line\":3,\"message\":\"Sensitive data detected - SECRET KEY\",\"severity\":\"High\"}]}\n\nPlease note, this is a simplified version of what you'd find in real-world codebases and could include false positives (false negatives are not included). We recommend running Semgrep or SonarQube on your codebase to get more accurate results. \n\nAlso please note that the output format may change based on how much data we have about the source code, so it's always better to start with a basic structure and add details as needed throughout the process of assessing security risks in your codebases.  \"\"\"\n    )\n\n    def __init__(self):\n        self._findings = []\n\n    @property\n    def findings(self) -> List[Dict]:\n        return self._findings\n\n    def find_sensitive_data(self, data: Dict) -> None:\n        for file_path in data[\"code\"]:\n            content = data[\"code\"][file_path]\n\n            # Check if the SECRET key is present. If yes, add it to findings list with severity High\n            secret_key_index = content.find(\"SECRET_KEY\")\n            if secret_key_index != -1:\n                self._add_finding(\n                    file_path=file_path, line=(secret_key_index + 3), message=\"Sensitive data detected - SECRET KEY\", severity=\"High\"\n                )\n\n    def _add_finding(self, file_path: str, line: int, message: str = \"\", severity: str = \"Medium\") -> None:\n        self._findings.append({\"file_path\": file_path, \"line\": line, \"message\": message, \"severity\": severity})\n\n    def process(self) -> Dict[str, List]:\n        data = json.loads(self.data)\n\n        # Extract sensitive data from source code and add findings to the list of findings\n        self.find_sensitive_data(data=data[\"code\"])\n\n        return {\"findings\": self._findings}\n\n\n# Test cases for this model\ndef test_case1():\n    input = '{\"code\":{\"../src/app.js\":\"// src/app.js\\r\\n\\r\\nconst SECRET_KEY = \\\"12345-ABCDE\\\";\\r\\n\\r\\nfunction getUserById(id) {\\r\\n    const query = \\\"SELECT * FROM users WHERE id = \\\" + id;\\r\\n}\\r\\n\\r\\nfunction executeCode(code) {\\r\\n    eval(code);\\r\\n}\\r\\n\\r\\nfunction add(a, b) {\\r\\n    return a + b;\\r\\n}\"},\"semgrep\":{},\"sonarqube\":{}}'\n    model = SensitiveDataDetectionModel()\n\n    output_data = json.loads(model.process(input))[\"findings\"]\n    assert len(output_data) == 1, \"Should find one finding for the SECRET key.\"\n\n\ndef test_case2():\n    input = '{\"code\":{\"../src/app.js\":\"// src/app.js\\r\\n\\r\\nconst SECRET_KEY = \\\"12345-ABCDE\\\";\\r\\n\\r\\nfunction getUserById(id) {\\r\\n    const query = \\\"SELECT * FROM users WHERE id = \\\" + id;\\r\\n}\\r\\n\\r\\nfunction executeCode(code) {\\r\\n    eval(code);\\r\\n}\\r\\n\\r\\nfunction add(a, b) {\\r\\n    return a + b;\\r\\n}\"},\"semgrep\":{},\"sonarqube\":{}}'\n    model = SensitiveDataDetectionModel()\n\n    output_data = json.loads(model.process(input))[\"findings\"]\n    assert len([f for f in output_data if f['file_path'] == '../src/app.js']) == 1, \"Should find one finding for the SECRET key.\"\n\n\ndef test_case3():\n    input = '{\"code\":{\"../src/app.js\":\"// src/app.js\\r\\n\\r\\nconst SECRET_KEY = \\\"12345-ABCDE\\\";\\r\\n\\r\\nfunction getUserById(id) {\\r\\n    const query = \\\"SELECT * FROM users WHERE id = \\\" + id;\\r\\n}\\r\\n\\r\\nfunction executeCode(code) {\\r\\n    eval(code);\\r\\n}\\r\\n\\r\\nfunction add(a, b) {\\r\\n    return a + b;\\r\\n}\"},\"semgrep\":{},\"sonarqube\":{}}'\n    model = SensitiveDataDetectionModel()\n\n    output_data = json.loads(model.process(input))[\"findings\"]\n    assert len([f for f in output_data if f['severity'] == 'High']) == 1, \"Should find one finding with High severity.\"\n\n\ntest_case1()\ntest_case2()\ntest_case3()\nprint(\"All tests passed\")\n\n# + id=\"O8qU7c04i9zG\" colab={\"base_uri\": \"https://localhost:8080/\"} outputId=\"6a5b2f0-1d9e-4d36-baee-a23861d9bd14\"\n# Test the model with a real example from SonarQube API\nimport requests, json\n\n\ndef test_with_real_example():\n    # Replace this URL with your own SonarQube server's REST API endpoint. Make sure to include authentication parameters if needed!\n    url = \"http://localhost:9000/api/issues/search?componentKeys=com.mycompany%3Amy-app&rules=squid%3AS125\"\n\n    response = requests.get(url)  # Send a GET request to the SonarQube API endpoint\n    if not response or response.status_code != 200:\n        print(\"Failed to retrieve data from SonarQube.\")\n        return\n\n    sonarqube_data = json.loads(response.text)[\"issues\"]  # Extract the issues list from the JSON response\n\n    model = SensitiveDataDetectionModel()\n    for issue in sonarqube_data:\n        file_path, line = f\"{issue['component']}/{issue['line']}\", int(issue[\"line\"])\n\n        message = \"Sensitive data detected - \" + issue[\"message\"] if 'description' in issue else \"\"\n        severity = {\"BLOCKER\": \"High\", \"CRITICAL\": \"Medium\", \"MAJOR\": \"Low\"}.get(issue[\"severity\"], \"\")  # Convert SonarQube's severity to our model's\n\n        model._add_finding(file_path=file_path, line=line, message=message, severity=severity)\n\n    output = json.dumps({\"findings\": model.process()[\"findings\"]})\n    print(\"Findings from real example:\")\n    print(output)\n\n\ntest_with_real_example()\n\n# + [markdown] id=\"yZ-lX3b4j0oF\"\n# # **Conclusion** \n# In this tutorial, you have learned how to use the `SensitiveDataDetectionModel` class for detecting sensitive data in source code. This model can be used as a starting point and can be extended to cover more types of security risks by adding rules or using machine learning techniques for better accuracy. You also saw an example on how to test the model with real-world data from SonarQube API.\n#\n# **Please note:** For production use, it's recommended to run Semgrep and SonarQube on your codebase instead of this simplified implementation. These tools can provide more accurate results at a lower cost and are widely used in industry for identifying security risks. \n",
      "logprobs": null,
      "finish_reason": "stop"
    }
  ],
  "usage": {
    "prompt_tokens": 203,
    "completion_tokens": 1731,
    "total_tokens": 1934
  }
}
